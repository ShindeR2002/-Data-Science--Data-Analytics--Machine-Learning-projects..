{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance using LSTM\n",
    "\n",
    "Deep learning has proven to show superior performance in certain domains such as object recognition and image classification. It has also gained popularity in domains such as finance where time-series data plays an important role. Predictive Maintenance is also a domain where data is collected over time to monitor the state of an asset with the goal of finding patterns to predict failures which can also benefit from certain deep learning algorithms. Among the deep learning methods, Long Short Term Memory (LSTM) networks are especially appealing to the predictive maintenance domain due to the fact that they are very good at learning from sequences. This fact lends itself to their applications using time series data by making it possible to look back for longer periods of time to detect failure patterns. In this notebook, we build an LSTM network for the data set and scenerio described at Predictive Maintenance Template to predict remaining useful life of aircraft engines. In summary, the template uses simulated aircraft sensor values to predict when an aircraft engine will fail in the future so that maintenance can be planned in advance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img2.cgtrader.com/items/2153687/4fb4f455c0/pw-gtf-geared-turbofan-engine-3d-model-max-obj-mtl-3ds-c4d-lwo-lw-lws-ma-mb.jpg\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing the dataset\n",
    "The dataset consists of sensor readings from a fleet of simulated aircraft gas turbine engines operating conditions as a multiple multivariate time series. The dataset consists of separate training and test sets. The testset is similar to the training set, except that each engine’s measurements are truncated some (unknown) amount of time before it fails. The data is provided as a ZIP-compressed text file with 26 columns of numbers. Each row represents a snapshot of data taken during a single operational cycle and each column represents a different variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/kagglesdsdata/datasets%2F675628%2F1187995%2Ffeatures.PNG?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1590650972&Signature=UTbUJ0kbk4z48slf6OnG3W5M0tbhrs3OhQDd%2BAZeWliS9B7QQFaRfDb3FB5NqiV0CQysvoJLXeBLuq4Ho5Ox1kXswON2duOlyQDHq9%2Feoz3aHDK3ykkOupydexrY3yc749keKgv47o6TM9CXT7pERyQ14KcuXGTMHM3XDGdzLSM480J8bPuU7U36X4zmOFELW1ctaMXqFJlgXZ5pttIV1aFiWKNLeDjaxokI46kzvF%2BMD%2FTvPRSrojaZ45sVfVYM5nvyT4qMpH2vWBe%2B5f6sUn1JKdrnW0gaI1zIAD5L0Vso%2FOMuXqJL5InY6MK6exyog7go62n%2B7zOMbZQyZ3uUSg%3D%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BsYPkY7gYGx"
   },
   "source": [
    "# Binary classification\n",
    "Predict if an asset will fail within certain time frame (e.g. cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnUoDmaskkDv"
   },
   "source": [
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfpzPSgG-If3",
    "outputId": "38d7e2d9-7822-4eec-9dd8-92e4dea814d0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'binary_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion\n",
    "In the following section, we ingest the training, test and ground truth datasets from azure storage. The training data consists of multiple multivariate time series with \"cycle\" as the time unit, together with 21 sensor readings for each cycle. Each time series can be assumed as being generated from a different engine of the same type. The testing data has the same data schema as the training data. The only difference is that the data does not indicate when the failure occurs. Finally, the ground truth data provides the number of remaining working cycles for the engines in the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTxUV3yVjqjZ"
   },
   "source": [
    " **Read training data - It is the aircraft engine run-to-failure data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjbfnUZGgc3C",
    "outputId": "81c9d81e-f813-4081-8fd0-7d7b01815ba5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = load_and_label_data('data/PM_train.csv')\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niazqZapkDN9"
   },
   "source": [
    "**Read test data - It is the aircraft engine operating data without failure events recorded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cMKLJ8aj983",
    "outputId": "543470af-d546-4867-f050-437bb45d5fca",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/pm-dataset/PM_test.txt', sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20ZPHj11kLaT"
   },
   "source": [
    "**Read ground truth data - It contains the information of true remaining cycles for each engine in the testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFp2WdMKj-c0",
    "outputId": "3f689bb6-5f89-434a-8827-ae9f5c4185cc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "truth_df = pd.read_csv('../input/pm-dataset/PM_truth.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "truth_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1P1KW6nkuW6"
   },
   "source": [
    "**Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure) for training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulY14O06knOI",
    "outputId": "dff44fa5-69ad-48bd-d2a5-85c56c4aaa98",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# Generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification, while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCqgckjKlfAH"
   },
   "source": [
    "**Next for testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbncVCr_lMAS",
    "outputId": "16a19f1e-39c9-4335-b7ed-78cd5613bf29",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MinMax normalization (from 0 to 1)\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsbGVr6rmSdL"
   },
   "source": [
    "**Function to reshape features into (samples, time steps, features)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSInZu-EkFtf",
    "outputId": "cde4d141-303d-4b0e-8341-8c0b00a2f84c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length.\"\"\"\n",
    "    # for one id we put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(\"seq_array shape =\",seq_array.shape)\n",
    "seq_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA6hfibtmg2o"
   },
   "source": [
    "**Function to generate labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VPdYrYXmY-6",
    "outputId": "ec4a4099-14fb-42bf-ca8a-9e19a90690f7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gen_labels(id_df, seq_length, label):\n",
    "    \n",
    "    # For one id we put all the labels in a single matrix.\n",
    "    \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "print(\"label_array shape =\",label_array.shape)\n",
    "label_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybV93NZEmt8a"
   },
   "source": [
    "**Next, we build a deep network.**\n",
    "\n",
    "**The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units.** \n",
    "\n",
    "**Dropout is also applied after each LSTM layer to control overfitting.** \n",
    "\n",
    "**Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.**\n",
    "\n",
    "Stacking LSTM hidden layers makes the model deeper, more accurately earning the description as a deep learning technique. \n",
    "The additional hidden layers are understood to recombine the learned representation from prior layers and create new representations at high levels of abstraction. \n",
    "\n",
    "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "\n",
    "The loss function binary crossentropy is used on yes/no decisions, e.g., multi-label classification. The loss tells you how wrong your model’s predictions are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efKPB9msmo-D",
    "outputId": "9f3a099d-79a1-4954-b5d7-5ae15f968a9c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# build the network\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyZJ2mP-1pB"
   },
   "source": [
    "## Model Evaluation on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpVYSzXkmk5l",
    "outputId": "27740c81-a7a4-43b1-ea94-19f46ff5d043",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy (Higher is better)')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_accuracy.png\")\n",
    "\n",
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss (Lower is better)')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))\n",
    "\n",
    "# make predictions and compute confusion matrix\n",
    "y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_true = label_array\n",
    "print(y_pred.shape)\n",
    "print(\"\\nPrediction on Validation Set is: \", y_pred)\n",
    "\n",
    "test_set = pd.DataFrame(y_pred)\n",
    "test_set.to_csv('binary_submit_train.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# compute precision and recall\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print( 'precision = ', precision, '\\n', 'recall = ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Model Evaluation on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yT96j4rkL0K",
    "outputId": "c015fff9-9a25-4121-de7c-40c8e19ea969",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We pick the last sequence for each id in the test data\n",
    "\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "#print(\"seq_array_test_last\")\n",
    "#print(seq_array_test_last)\n",
    "#print(seq_array_test_last.shape)\n",
    "\n",
    "# Similarly, we pick the labels\n",
    "\n",
    "# it is used to take only the labels of the sequences that are at least 50 long\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "print(\"y_mask\")\n",
    "print(len(y_mask))\n",
    "label_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "print(label_array_test_last.shape)\n",
    "#print(\"label_array_test_last\")\n",
    "#print(label_array_test_last)\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)\n",
    "\n",
    "# test metrics\n",
    "scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
    "print('Accurracy: {}'.format(scores_test[1]))\n",
    "\n",
    "# make predictions and compute confusion matrix\n",
    "y_pred_test = estimator.predict_classes(seq_array_test_last)\n",
    "y_true_test = label_array_test_last\n",
    "print(y_pred.shape)\n",
    "\n",
    "test_set = pd.DataFrame(y_pred_test)\n",
    "test_set.to_csv('binary_submit_test.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "print(cm)\n",
    "\n",
    "# compute precision and recall\n",
    "precision_test = precision_score(y_true_test, y_pred_test)\n",
    "recall_test = recall_score(y_true_test, y_pred_test)\n",
    "f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
    "print( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )\n",
    "\n",
    "# Plot in blue color the predicted data and in green color the\n",
    "# actual data to verify visually the accuracy of the model.\n",
    "fig_verify = plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_pred_test, color=\"blue\")\n",
    "plt.plot(y_true_test, color=\"green\")\n",
    "plt.title('prediction')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('row')\n",
    "plt.legend(['predicted', 'actual data'], loc='upper left')\n",
    "plt.show()\n",
    "fig_verify.savefig(\"model_verify.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElhakcHtnX9J"
   },
   "source": [
    "# Regression\n",
    "How many more cycles an in-service engine will last before it fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWuxznPhATyL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'regression_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LF8N3B5fAZT2"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxowlCjhnkId",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read training data - It is the aircraft engine run-to-failure data.\n",
    "train_df = pd.read_csv('../input/pm-dataset/PM_train.txt', sep=\" \", header=None)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
    "test_df = pd.read_csv('../input/pm-dataset/PM_test.txt', sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
    "truth_df = pd.read_csv('../input/pm-dataset/PM_truth.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmS3G0OnAegq"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRfcnQH8nqII",
    "outputId": "14d5042b-c962-459b-9096-b8b872df1a96",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure) for training data\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification, \n",
    "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "\n",
    "\n",
    "# Next to testing data\n",
    "# MinMax normalization (from 0 to 1)\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(test_df.head())\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzGdxBMsuePI",
    "outputId": "f622ba60-409e-424b-f2e2-1758ca09f55c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length.\"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,142),(50,192)\n",
    "\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "\n",
    "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
    "val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
    "print(len(val))\n",
    "\n",
    "# generator for the sequences\n",
    "# transform each id of the train dataset in a sequence\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n",
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ps8UixLAj1D"
   },
   "source": [
    "## LSTM\n",
    "\n",
    "The coefficient of determination R2 can describe how “good” a model is at making predictions: it represents the proportion of the variance in the dependent variable that is predictable from the independent\n",
    "\n",
    "mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon\n",
    "\n",
    "Loss function Mean squared error (MSE) is the most commonly used loss function for regression. The loss is the mean overseen data of the squared differences between true and predicted values.\n",
    "\n",
    "RmsProp is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients. We always keep a moving average over the root mean squared (hence Rms) gradients, by which we divide the current gradient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPx1AzOan6VT",
    "outputId": "b17ae812-228c-42a2-93da-5e8c88a72e83",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination \n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_AqerQKAsaA"
   },
   "source": [
    "## Model Evaluation on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNHApglLn-CT",
    "outputId": "e1e51dc3-0079-40cf-9ce2-83dbae6322d4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# summarize history for R^2\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['r2_keras'])\n",
    "plt.plot(history.history['val_r2_keras'])\n",
    "plt.title('model r^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_r2.png\")\n",
    "\n",
    "# summarize history for MAE\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_mae.png\")\n",
    "\n",
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_regression_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('\\nMAE: {}'.format(scores[1]))\n",
    "print('\\nR^2: {}'.format(scores[2]))\n",
    "\n",
    "y_pred = model.predict(seq_array,verbose=1, batch_size=200)\n",
    "y_true = label_array\n",
    "\n",
    "test_set = pd.DataFrame(y_pred)\n",
    "test_set.to_csv('submit_train.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-4BShGEAyMP"
   },
   "source": [
    "## Evaluate on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDrp7F6Fn-3p",
    "outputId": "c0184cac-8325-4f80-9121-ca16eed9b5b7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We pick the last sequence for each id in the test data\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "#print(\"seq_array_test_last\")\n",
    "#print(seq_array_test_last)\n",
    "#print(seq_array_test_last.shape)\n",
    "\n",
    "# Similarly, we pick the labels\n",
    "#print(\"y_mask\")\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "#print(label_array_test_last.shape)\n",
    "#print(\"label_array_test_last\")\n",
    "#print(label_array_test_last)\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path,custom_objects={'r2_keras': r2_keras})\n",
    "\n",
    "    # test metrics\n",
    "    scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
    "    print('\\nMAE: {}'.format(scores_test[1]))\n",
    "    print('\\nR^2: {}'.format(scores_test[2]))\n",
    "\n",
    "    y_pred_test = estimator.predict(seq_array_test_last)\n",
    "    y_true_test = label_array_test_last\n",
    "\n",
    "    test_set = pd.DataFrame(y_pred_test)\n",
    "    test_set.to_csv('submit_test.csv', index = None)\n",
    "    test_set= test_set.apply(np.ceil)\n",
    "\n",
    "    # Plot in blue color the predicted data and in green color the\n",
    "    # actual data to verify visually the accuracy of the model.\n",
    "    fig_verify = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(y_pred_test, color=\"blue\")\n",
    "    plt.plot(y_true_test, color=\"green\")\n",
    "    plt.title('prediction')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('row')\n",
    "    plt.legend(['predicted', 'actual data'], loc='upper left')\n",
    "    plt.show()\n",
    "    fig_verify.savefig(\"model_regression_verify.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1T8y5qA91x8"
   },
   "source": [
    "# References\n",
    "\n",
    "\n",
    "- [1] Deep Learning for Predictive Maintenance https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb\n",
    "- [2] Predictive Maintenance: Step 2A of 3, train and evaluate regression models https://gallery.cortanaintelligence.com/Experiment/Predictive-Maintenance-Step-2A-of-3-train-and-evaluate-regression-models-2\n",
    "- [3] A. Saxena and K. Goebel (2008). \"Turbofan Engine Degradation Simulation Data Set\", NASA Ames Prognostics Data Repository (https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan), NASA Ames Research Center, Moffett Field, CA \n",
    "- [4] Understanding LSTM Networks http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
